# Variational Autoencoder for Even Numbers

This repository contains the implementation of a Variational Autoencoder (VAE) designed to learn and generate even numbers from the MNIST dataset. The VAE uses convolutional units in its encoder for better feature extraction.

## Table of Contents

- [Introduction](#introduction)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
  - [Training](#training)
  - [Generating Samples](#generating-samples)
- [Key Functions and Classes](#key-functions-and-classes)
  - [VAE Class](#vae-class)
  - [`train_vae` Function](#train_vae-function)
  - [`generate_samples` Function](#generate_samples-function)
  - [`loss_function` Function](#loss_function-function)
  - [`plot_loss` Function](#plot_loss-function)
- [Results](#results)
- [Dependencies](#dependencies)
- [Use of AI](#use-of-ai)


## Introduction

The VAE architecture consists of a convolutional encoder and decoder, with a specified latent space dimensionality. The model is trained on a downsized version of the MNIST dataset containing only even numbers.

## Getting Started

### Prerequisites

- PyTorch
- Matplotlib
- torchvision

### Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/chitwan-waterloo/PHYS449.git
    ```

2. Navigate to the project directory:

    ```bash
    cd 'PHYS449/Assignment 3'
    ```

3. Install the required dependencies:

    ```bash
    pip install -r requirements.txt
    ```

## Usage

### Training and Generating Samples

To train the VAE and genrate sample images, use the following command:

```bash
python main.py -o result_dir -n 100 --epochs 50 --verbose --latent_dim 2
```

- `-o` or `--output_dir`: Specify the output directory for result files.
- `-n` or `--num_samples`: Number of samples to generate.
- `--epochs`: Number of training epochs.
- `--verbose`: Enable verbose mode.
- `--latent_dim`: Dimensionality of the latent space.

## Key Functions and Classes

### VAE Class

The `VAE` class represents the Variational Autoencoder in this project. It consists of an encoder and decoder, providing a powerful tool for learning and generating even numbers from the MNIST dataset.

### `train_vae` Function

The `train_vae` function is responsible for training the VAE model. It takes input data, the number of epochs, and other parameters, and iteratively optimizes the model's parameters.

### `generate_samples` Function

The `generate_samples` function uses the trained VAE model to generate a specified number of sample images. These images are saved in the output directory for later analysis.

### `loss_function` Function

The `loss_function` function calculates the VAE loss, which is a combination of Mean Squared Error (MSE) and Kullback-Leibler Divergence (KLD) terms.

### `plot_loss` Function

The `plot_loss` function plots and saves the training loss over epochs.


## Results

After training, the following files will be saved in the specified output directory:

1. `loss.pdf`: Figure showing the progress of the VAE in training.
2. `1.pdf`, `2.pdf`, ..., `100.pdf`: One hundred digit sample images generated by the trained model.

## Dependencies

torch==1.9.1
matplotlib==3.4.3
torchvision==0.10.1

## Use of AI

This project utilizes PyTorch, a popular deep learning library, for the implementation of a Variational Autoencoder (VAE). The VAE is a type of artificial intelligence model used for generative tasks, specifically in this case for generating even numbers from the MNIST dataset.